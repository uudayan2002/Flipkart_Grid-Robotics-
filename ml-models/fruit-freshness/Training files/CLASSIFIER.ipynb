{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a feature extracting kernel made for increasing accuracy\n",
    "def extraction (image):\n",
    "    Kernel  = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    img_k = cv2.filter2D(image,-1, kern)\n",
    "    img = cv2.cvtColor(img_k, cv2.COLOR_BGR2HSV)\n",
    "    img[:,:,1] = 255\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Datasets\n",
    "\n",
    "datadir = r\"C:\\Users\\LG\\Desktop\\Ripe Fruit Detector\"\n",
    "Categories = ['Ripe', 'Unripe']\n",
    "training_data = []\n",
    "test_images = ['Test']\n",
    "test_data = []\n",
    "def train ():\n",
    "\n",
    "    for category in Categories:\n",
    "        path = os.path.join(datadir, category)\n",
    "        class_num = Categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img))\n",
    "                img = cv2.cvtColor(img_array, cv2.COLOR_BGR2HSV)\n",
    "                img[:,:,1] = 255\n",
    "                \n",
    "                img_array = cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n",
    "                new_array = cv2.resize(img_array, (100, 100))\n",
    "                # here i changed flatten to reshaped (1, -1)\n",
    "                image = np.array(new_array).flatten()\n",
    "                # image = np.array(image).reshape(1,-1)\n",
    "                training_data.append([image, class_num])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def test ():\n",
    "    for category in test_images:\n",
    "        path = os.path.join(datadir, category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img))\n",
    "\n",
    "                img_hsv = cv2.cvtColor(img_array, cv2.COLOR_BGR2HSV)\n",
    "                img_hsv[:, :, 1] = 255\n",
    "                img_array = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)\n",
    "                new_array = cv2.resize(img_array, (100, 100))\n",
    "                # here i changed flatten to reshaped (1, -1)\n",
    "                image = np.array(new_array).flatten()\n",
    "#                 image = np.array(image).reshape(1,-1)\n",
    "\n",
    "                test_data.append([image])\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "#TRAINING DATA PREPARATION \n",
    "train()\n",
    "random.shuffle(training_data)\n",
    "print(len(training_data))\n",
    "with open('training_data.pkl', 'wb') as f:\n",
    "    pickle.dump(training_data,f)\n",
    "print ('Saved Successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Attributes Successfully!\n",
      "Saved labels Successfully!\n"
     ]
    }
   ],
   "source": [
    "### SPLITTING DATA INTO ATTRIBUTES AND LABELS ###\n",
    "\n",
    "with open('training_data.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for features, label in training_data:\n",
    "    x.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "\n",
    "\n",
    "with open('x.pkl', 'wb') as feat:\n",
    "    pickle.dump(x,feat)\n",
    "print ('Saved Attributes Successfully!')\n",
    "\n",
    "with open('y.pkl', 'wb') as lab:\n",
    "    pickle.dump(y,lab)\n",
    "print ('Saved labels Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8888888888888888\n",
      "Overfit:  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90        37\n",
      "           1       0.81      0.96      0.88        26\n",
      "\n",
      "    accuracy                           0.89        63\n",
      "   macro avg       0.89      0.90      0.89        63\n",
      "weighted avg       0.90      0.89      0.89        63\n",
      "\n",
      "[[31  6]\n",
      " [ 1 25]]\n"
     ]
    }
   ],
   "source": [
    "###### LOADING Training dataset for splitting ####\n",
    "\n",
    "with open('x.pkl', 'rb')as feat:\n",
    "    X = pickle.load(feat)\n",
    "with open('y.pkl ', 'rb') as lab:\n",
    "    Y = pickle.load(lab)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TRAINING Time bro ###\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "\n",
    "# xtrain = sc.fit_transform(xtrain)\n",
    "# xtest = sc.transform(xtest)\n",
    "\n",
    "model = SVC(C=1 , kernel = 'linear', gamma='auto')\n",
    "# model = CalibratedClassifierCV(model)\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "Accuracy = model.score(xtest, ytest)\n",
    "overfit = model.score(xtrain, ytrain)\n",
    "predictions = model.predict(xtest)\n",
    "print ('Accuracy: ', Accuracy)\n",
    "print ('Overfit: ', overfit)\n",
    "print(classification_report(ytest, predictions ))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(ytest, predictions)\n",
    "print (cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn . ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = BaggingClassifier(SVC(C=1 , kernel = 'linear', gamma='auto', probability = True), max_samples = 1.0 , max_features = 1.0, n_estimators = 30, oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=SVC(C=1, break_ties=False, cache_size=200,\n",
       "                                     class_weight=None, coef0=0.0,\n",
       "                                     decision_function_shape='ovr', degree=3,\n",
       "                                     gamma='auto', kernel='linear', max_iter=-1,\n",
       "                                     probability=True, random_state=None,\n",
       "                                     shrinking=True, tol=0.001, verbose=False),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=30, n_jobs=None, oob_score=True,\n",
       "                  random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg.fit(xtrain , ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888888888888888\n",
      "Overfit:  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90        37\n",
      "           1       0.81      0.96      0.88        26\n",
      "\n",
      "    accuracy                           0.89        63\n",
      "   macro avg       0.89      0.90      0.89        63\n",
      "weighted avg       0.90      0.89      0.89        63\n",
      "\n",
      "[[31  6]\n",
      " [ 1 25]]\n"
     ]
    }
   ],
   "source": [
    "print(bg.score (xtest, ytest))\n",
    "bg.predict(xtest)\n",
    "overfit = bg.score(xtrain, ytrain)\n",
    "predictions = bg.predict(xtest)\n",
    "# print ('Accuracy: ', Accuracy)\n",
    "print ('Overfit: ', overfit)\n",
    "print(classification_report(ytest, predictions ))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(ytest, predictions)\n",
    "print (cm)\n",
    "# print(bg.score (xtrain, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessing the model\n",
    "clf = cross_val_score(bg, X, Y, cv = 10)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87234043, 0.91489362, 0.82608696, 0.7826087 , 0.82608696,\n",
       "       0.84782609, 0.80434783, 0.95652174, 0.84782609, 0.91304348])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = cross_val_score(bg, X, Y, cv = 10)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81132075, 0.77358491, 0.86792453, 0.8490566 , 0.83018868,\n",
       "       0.90566038, 0.83018868, 0.75471698, 0.8490566 , 0.83018868])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = cross_val_score(model, X, Y, cv = 10)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dt = DecisionTreeClassifier(max_depth = 2)\n",
    "# dt.fit(xtrain, ytrain)\n",
    "# dt.score(xtrain, ytrain)\n",
    "rf = RandomForestClassifier(n_estimators = 11)\n",
    "rf.fit(xtrain, ytrain)\n",
    "rf.score(xtest, ytest)\n",
    "# rf.score(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.score(xtest , ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C': [1,1.1,1.5, 2,10 ],'gamma': ['scalar', 'auto'],'kernel': ['linear','poly'] }, \n",
    "                {'C': [1,1.1,1.5, 2, 10 ],'gamma': [0.1,0.2,0.3,0.4, 0.5, 1 ],'kernel': ['rbf']}, ]\n",
    "\n",
    "gridsearch = GridSearchCV(estimator = model, param_grid= parameters, scoring = 'accuracy', cv = 10, n_jobs = 2)\n",
    "gridsearch = gridsearch.fit(xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'gamma': 'auto', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pineapple Successfully!\n"
     ]
    }
   ],
   "source": [
    "### Saving the damn model ###\n",
    "with open('Banana.pkl', 'wb') as mod:\n",
    "    pickle.dump(bg, mod)\n",
    "print (f'Saved pineapple Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing images to confirm stuff, i.e importing the image directly ###\n",
    "img = cv2.imread('IMG_20201218_171647_152.jpg')\n",
    "img = cv2.resize(img, (100,100))\n",
    "img = np.array(img).reshape(1,-1)\n",
    "\n",
    "cv2.imshow('test',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "#### for testing if you have multiple real world datasets ####\n",
    "test_data = []\n",
    "test()\n",
    "print(len(test_data))\n",
    "with open('real.pkl', 'wb') as t:\n",
    "    pickle.dump(test_data,t)\n",
    "print ('Saved Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal model:  Unripe [[0.18744144 0.81255856]]\n",
      "Normal model:  Unripe [[0.29464335 0.70535665]]\n",
      "Normal model:  Ripe [[0.90884104 0.09115896]]\n"
     ]
    }
   ],
   "source": [
    "# with open('Orange Model_sat.pkl', 'rb') as moda:\n",
    "#     model = pickle.load(moda)\n",
    "# with open('banana_v1.pkl', 'rb') as mod:\n",
    "\n",
    "#     model1 = pickle.load(mod)\n",
    "    \n",
    "    \n",
    "with open('real.pkl', 'rb') as ril:\n",
    "    real = pickle.load(ril)\n",
    "\n",
    "for p in real:\n",
    "    p = np.array(p).reshape(1,-1)\n",
    "    \n",
    "    # p.reshape(1,-1)\n",
    "\n",
    "#     print('+data model: ', Categories[model1.predict(p)[0]],model1.predict_proba(p) )\n",
    "#     print('Proba model: ', Categories[model.predict(p)[0]],model.predict_proba(p) )\n",
    "    print('Normal model: ', Categories[bg.predict(p)[0]],bg.predict_proba(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chai = cv2.imread(\"IMG_20210111_172305.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "omo =cv2.resize(chai, (100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('omoooo', omo)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import bz2\n",
    "import _pickle as cPickle\n",
    "with open('Orange.pkl', 'rb') as moda:\n",
    "    model = pickle.load(moda)\n",
    "with open('Pineapple.pkl','rb') as pine:\n",
    "    model0 = pickle.load(pine)\n",
    "with open('Banana.pkl','rb') as ban:\n",
    "    model1 = pickle.load(ban)\n",
    "# Pickle a file and then compress it into a file with extension \n",
    "def compressed_pickle(title, data):\n",
    "    with bz2.BZ2File(title + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(data, f)\n",
    "compressed_pickle('orange', model)\n",
    "compressed_pickle('pineapple', model0)\n",
    "compressed_pickle('banana', model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proba model:  Ripe [[0.78785347 0.21214653]]\n",
      "Proba model:  Ripe [[0.9124426 0.0875574]]\n",
      "Proba model:  Unripe [[0.04711674 0.95288326]]\n",
      "Proba model:  Ripe [[0.55816224 0.44183776]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import bz2\n",
    "import _pickle as cPickle\n",
    "\n",
    "# Load any compressed pickle file\n",
    "# def decompress_pickle(file):\n",
    "#     data = bz2.BZ2File(file, 'rb')\n",
    "#     data = cPickle.load(data)\n",
    "#     return data\n",
    "# model  = decompress_pickle('banana.pbz2')\n",
    "\n",
    "with open('real.pkl', 'rb') as ril:\n",
    "    real = pickle.load(ril)\n",
    "for p in real:\n",
    "    p = np.array(p).reshape(1,-1)\n",
    "    \n",
    "    # p.reshape(1,-1)\n",
    "#     print('+data model: ', Categories[model1.predict(p)[0]],model1.predict_proba(p) )\n",
    "    print('Proba model: ', Categories[model.predict(p)[0]],model.predict_proba(p) )\n",
    "#     print('Normal model: ', Categories[bg.predict(p)[0]],bg.predict_proba(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
